--- ./mm/mmap.c.orig	2007-09-20 14:49:49.000000000 +0200
+++ ./mm/mmap.c	2007-09-20 17:13:56.000000000 +0200
@@ -1435,6 +1435,7 @@ static int over_stack_limit(unsigned lon
 int expand_stack(struct vm_area_struct * vma, unsigned long address)
 {
 	unsigned long grow;
+	unsigned long size;
 
 	if (!(vma->vm_flags & VM_GROWSUP))
 		return -EFAULT;
@@ -1454,6 +1455,7 @@ int expand_stack(struct vm_area_struct *
 	 */
 	address += 4 + PAGE_SIZE - 1;
 	address &= PAGE_MASK;
+	size = address - vma->vm_start;
 	grow = (address - vma->vm_end) >> PAGE_SHIFT;
 	
 	/* Someone beat us to it */
@@ -1461,6 +1463,13 @@ int expand_stack(struct vm_area_struct *
 		anon_vma_unlock(vma);
 		return 0;
 	}
+
+        /* Check to ensure the stack will not grow into a hugetlb-only region */
+        if (is_hugepage_only_range(vma->vm_start, size)) {
+		anon_vma_unlock(vma);
+                return -EFAULT;
+	}
+
 	/* Overcommit.. */
 	if (security_vm_enough_memory(grow)) {
 		anon_vma_unlock(vma);
@@ -1513,6 +1522,7 @@ find_extend_vma(struct mm_struct *mm, un
 int expand_stack(struct vm_area_struct *vma, unsigned long address)
 {
 	unsigned long grow;
+	unsigned long size;
 
 	/*
 	 * We must make sure the anon_vma is allocated
@@ -1528,6 +1538,7 @@ int expand_stack(struct vm_area_struct *
 	 * anon_vma lock to serialize against concurrent expand_stacks.
 	 */
 	address &= PAGE_MASK;
+	size = vma->vm_end - address;
 	grow = (vma->vm_start - address) >> PAGE_SHIFT;
 
 	/* Someone beat us to it */
@@ -1535,6 +1546,13 @@ int expand_stack(struct vm_area_struct *
 		anon_vma_unlock(vma);
 		return 0;
 	}
+
+        /* Check to ensure the stack will not grow into a hugetlb-only region */
+        if (is_hugepage_only_range(vma->vm_end - size, size)) {
+		anon_vma_unlock(vma);
+                return -EFAULT;
+	}
+
 	/* Overcommit.. */
 	if (security_vm_enough_memory(grow)) {
 		anon_vma_unlock(vma);
